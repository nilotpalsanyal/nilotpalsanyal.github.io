---
title: "Matrix formulae"
author: "Nilotpal Sanyal, Ph.D.  <br><br> [https://nilotpalsanyal.github.io](https://nilotpalsanyal.github.io)"
date: 
output: 
 html_document:
  toc: yes
  toc_depth: 3  
  toc_float: TRUE        
  number_sections: no   
  # theme: united     
  highlight: tango   
urlcolor: blue
citecolor: blue
---

<!-- # set the overall width of the HTML page -->
<style type="text/css"> /* This sets the overall width of the HTML page */
  .main-container {
    /*max-width: 600px;   */
    margin-left: auto;
    margin-right: 10%;
  }
  body, td {
     font-size: 18px;
     /*font-family: Calibri;*/
     background: rgb(250,250,250);
  }
  code.r{                   /*for r code*/
    font-size: 16.5px;
  }
  pre {                 /*for output of knitr chunks*/
    font-size: 16.5px
    border: 0;
  }
  #TOC {
  color: purple;
  font-size: 15px; 
  }
  }
</style>

\usepackage{amsmath}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\colsp}{colsp}
\DeclareMathOperator{\rowsp}{rowsp}
\DeclareMathOperator{\nullsp}{nullsp}
\DeclareMathOperator{\trace}{trace}


<hr>

Good reference: Seber - A Matrix Handbook for Statististicians

### Rank

- $\rank(A) = \rank(A^T) = r$, the number of linearly independent columns/rows in $A$.

- $\rank(A^{m\times n}) \leq \min(m,n)$.

- For real $A$, $\rank(A^TA)=\rank(AA^T)=\rank(A)$.

- If $\rank(A)=0$, then $A=0$. If $\rank(A)=1$, then there exist nonzero $a$ and $b$ such that $A = ab^T$.

- $\rank(A^{m\times n}B^{n\times p}) \leq \min(\rank(A^{m\times n}),\rank(B^{n\times p})) \leq \min(m,n,p)$. 

- If $\rank(A^{m\times n}) = n$, then $\rank(A^{m\times n}B^{n\times p}) = \rank(B^{n\times p})$.

- If $\rank(B^{n\times p}) = n$, then $\rank(A^{m\times n}B^{n\times p}) = \rank(A^{m\times n})$.

- $\rank(A+B) \leq \rank(A)+\rank(B)$.

- $\rank(A) = \rank(A^T) = \rank(A^TA) = \rank(AA^T)$.

- $A^{n\times n}$ is invertible iff $\rank(A) = n$.

- **Full rank factorization**: For any $A^{m\times n}$ with rank $r$, $A^{m\times n} = C^{m\times r}D^{r\times n}$ where $C$ and $D$ are of full rank $r$.

- Iff $C^{m\times m}$ and $D^{n\times n}$ are nonsingular, $\rank(CA^{m\times n}D) = \rank(A^{m\times n})$.

- If $\colsp(A) = \colsp(B)$, then $\rank(CA) = \rank(CB)$ for all $C$.

- If $\rank(A^{p\times q})=q$ and $\rank(B^{q\times r})=r$, then $\rank(AB)=r$.

- $\rank(A-B) \geq |\rank(A) - \rank(B)|$.


$$\\[0.1in]$$




### Column/row space

- Column space, or range, or image: $\colsp(A^{m\times n}) = \{ A^{m\times n}x^{n\times 1} \}$, the set of all linear combinations of the columns of $A$.

- Row space: $\rowsp(A^{m\times n}) = \colsp({A^T}^{n\times m})$, the set of all linear combinations of the rows of $A$.

- Null space: $\nullsp(A^{m\times n}) = \{x^{n\times 1}: Ax=0\}$, the collection of all vectors which are taken to $0$ by the transformation $A$.

- The following statements are equivalent:

  - There exists a solution to the equation $Ax = b$.
  - $b \in \colsp(A)$ (some linear combination of the columns of $A$ must be $b$).
  - $\rank(A) = \rank([A \; b])$ (as $b$ is a linear combination of the columns of $A$, concatenating $b$ to $A$ does not change the rank of $A$).

- The following statements are equivalent:

  - Solutions to the equation $Ax = b$ are unique.
  - $\nullsp(A)=\{0\}$.
  - $\rank(A) = n$.

- $\rank(A)+\dim(\nullsp(A))=n$. $\dim(\nullsp(A))$ is called **nullity** of $A$.

$$\\[0.1in]$$




### Orthoginal matrix

- $A^{m\times n}$ with $m\geq n$ is orthogonal if $A^TA=I$. If $m=n$, $A^TA=AA^T=I$ and $A^{-1}=A^T$.

- For two orthogonal matrices $A$ and $B$, $\|AUB^T\|_p=\|U\|_p (such norms are invariant)$.

- For any $A^{m\times n}$, there exists orthogonal $B^{m\times r}$ with $r=\rank(A)$ such that $\colsp(A)=\colsp(B)$.

$$\\[0.1in]$$




### Idempotent matrix

- $A$ is idempotent if $A^2 = A$ ($A$ must be a square matrix).

$$\\[0.1in]$$




### Singular Value Decomposition (SVD)

- Any $A^{m\times n}$ can be decomposed as 
$$A^{m\times n}=U^{m\times r}\Sigma^{r\times r}{V^T}^{r\times n}$$
where $U$ and $V$ are orthogonal and $\Sigma$ is diagonal with elements $\sigma_1 \geq \ldots \geq \sigma_r > 0$ called the **singular values**. The columns of $U$, $u_i$s, are called the **left singular vectors** and of $V$, $v_i$s, are called the **right singular vectors**. It can be converted to full SVD as 
$$A^{m\times n} = [U \; U_1]^{m\times m} 
\begin{bmatrix} \Sigma & 0 \\ 0 & 0 \end{bmatrix}^{m\times n}
\begin{bmatrix} V^T \\ V_1^T \end{bmatrix}^{n\times n}.
$$

  - $Av_i = \sigma_i u_i$ and $A^Tu_i = \sigma_iv_i$.
  - For full SVD, $\rank(A)=r$ and $A^T = V\Sigma U^T$.


$$\\[0.1in]$$




### Trace

- $\trace(A)=\sum_{i=1}^n a_{ii}$.

- For real and symmetruc $A$, $$\rank(A) \geq \frac{(\trace(A))^2}{\trace(A^2)}.$$

- If $A^{n\times n}$ is a real matrix with real eigenvalues exactly $t$ of which are nonzero, then $$(\trace(A))^2 \leq t\trace(A^2).$$

- For $A^{m\times n}$ and $B^{n\times m}$, 

  - $\trace(AB)=\trace(BA)=\trace(A^TB^T)=\trace(B^TA^T) = \sum_{i=1}^m\sum_{j=1}^n a_{ij}b_{ji}$.

  - If $m=n$ and either of $A$ or $B$ is symmetric, then $\trace(AB)=\sum_{i=1}^m\sum_{j=1}^n a_{ij}b_{ij}$.<br><br>

- For $A^{m\times n}$, $B^{n\times p}$ and $C^{p\times n}$, then $\trace(ABC)=\trace(BCA)=\trace(CAB)$.

- If $A$ and $B$ are real and symmetric, then $\trace[(AB)^2] \leq \trace(A^2B^2)$.

- $\trace(A \otimes B) = \trace(A) \trace(B)$.

- If $A$ is non-negative definite, $\trace(A) \geq 0$ wit equality iff $A=0$. If $A$ is positive definite, $\trace(A)>0$.

$$\\[0.1in]$$




### Determinants

- $\det(A^{n\times n}B^{n\times n}) = \det(A) \det(B).$

- $\det \begin{bmatrix}A \; B \\ B \; A \end{bmatrix} = \det(A+B)\det(A-B).$


### Differentiation and integration of determinants

If 
$$
\Delta(x) =
\begin{vmatrix}
f_1(x) & g_1(x) \\ 
f_2(x) & g_2(x)
\end{vmatrix}
$$
then
$$
\Delta'(x) =
\begin{vmatrix}
f'_1(x) & g'_1(x) \\ 
f_2(x) & g_2(x)
\end{vmatrix} + 
\begin{vmatrix}
f_1(x) & g_1(x) \\ 
f'_2(x) & g'_2(x)
\end{vmatrix} =
\begin{vmatrix}
f'_1(x) & g_1(x) \\ 
f'_2(x) & g_2(x)
\end{vmatrix} + 
\begin{vmatrix}
f_1(x) & g'_1(x) \\ 
f_2(x) & g'_2(x)
\end{vmatrix},
$$

and

$$
\int \Delta(x)dx =
\begin{vmatrix}
\int f_1(x)dx & \int g_1(x)dx \\ 
f_2(x) & g_2(x)
\end{vmatrix} + 
\begin{vmatrix}
f_1(x) & g_1(x) \\ 
\int f_2(x)dx & \int g_2(x)dx
\end{vmatrix} =
\begin{vmatrix}
\int f_1(x)dx & g_1(x) \\ 
\int f_2(x)dx & g_2(x)
\end{vmatrix} + 
\begin{vmatrix}
f_1(x) & \int g_1(x)dx \\ 
f_2(x) & \int g_2(x)dx
\end{vmatrix}.
$$

$$\\[0.1in]$$
















$$\\[0.1in]$$




### 






















