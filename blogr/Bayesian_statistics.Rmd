---
title: "Bayesian Statistics"
author: "Nilotpal Sanyal, Ph.D.  <br><br> [https://nilotpalsanyal.github.io](https://nilotpalsanyal.github.io)"
date: 
output: 
 html_document:
  toc: yes
  toc_depth: 3  
  toc_float: TRUE        
  number_sections: no   
  # theme: united     
  highlight: tango   
urlcolor: blue
citecolor: blue
---

<!-- # set the overall width of the HTML page -->
<style type="text/css"> /* This sets the overall width of the HTML page */
  .main-container {
    /*max-width: 600px;   */
    margin-left: auto;
    margin-right: 10%;
  }
  body, td {
     font-size: 17px;
     /*font-family: Calibri;*/
     background: rgb(250,250,250);
  }
  code.r{                   /*for r code*/
    font-size: 16.5px;
  }
  pre {                 /*for output of knitr chunks*/
    font-size: 16.5px
    border: 0;
  }
  #TOC {
  color: purple;
  font-size: 15px; 
  }
  }
</style>

\usepackage{amsmath}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\colsp}{colsp}
\DeclareMathOperator{\rowsp}{rowsp}
\DeclareMathOperator{\nullsp}{nullsp}
\DeclareMathOperator{\trace}{trace}
\usepackage{physics}   


# Deterministic approximations

## Normal approximation

## Laplace approximation

Laplace approximation approximates any unimodal distribution with a suitable Normal distribution.

For a function $f(x)$, using the Taylor expansion

$$\ln f(x) = \ln f(x_0) + \frac{\partial \ln f(x)}{\partial x}\biggr\rvert_{x=x_0}(x-x_0) + {1\over 2!}\frac{\partial^2 \ln f(x)}{\partial x^2}\biggr\rvert_{x=x_0}(x-x_0)^2 + \ldots.$$

Taking $x_0=x_{max}$, a local maxima and ignoring higher-order terms as negligible, we have (as the partial derivative in the second term becomes 0 at $x_{max}$)
$$\ln f(x) = \ln f(x_{max}) + {1\over 2}\frac{\partial^2 \ln f(x)}{\partial x^2}\biggr\rvert_{x=x_{max}}(x-x_{max})^2 + \ldots.$$

Exponentiating both sides, 
$$f(x) = f(x_{max}) \exp\left[{1\over 2}\frac{\partial^2 \ln f(x)}{\partial x^2}\biggr\rvert_{x=x_{max}}(x-x_{max})^2 \right] \; \propto \; N\left(x_{max}, -\frac{1}{\frac{\partial^2 \ln f(x)}{\partial x^2}\biggr\rvert_{x=x_{max}}} \right).$$

**Reference**: <br>
Luke Tierney and Joseph Kadane. Accurate approximations for posterior moments and marginal densities. Journal of the American Statistical Association, 81(393), 1986.

$$\\[0.1in]$$




## Structured additive regression (STAR) models

Structured additive regression (STAR) models are a rich class of regression models that include the generalized linear model (GLM) and the generalized additive model (GAM). STAR models can be fitted by Bayesian approaches, component-wise gradient boosting, penalized least-squares, and deep learning. 

Given the response $Y^{n\times 1}$ and covariates $X^{n\times p}$, it is of the form
$$g(Y | X=x) = f(x) = \sum_{j=1}^p f_j(x),$$
where $g(.)$ is any function and each $f_j(x)$ uses a subset of the $p$ covariates.

Important special cases are the generalized linear model (GLM) [Nelder and Wedderbern, 1972]
$$
g(E(Y|X=x)) = \beta_0 + \beta_1 x_1 + ... + \beta_p x_p,
$$

and the generalized additive model (GAM) without interactions [Hastie and Tibshirani, 1986; Hastie and Tibshirani, 1990; Wood, 2017)
$$
g(E(Y|X=x)) = \beta_0 + f_1(x_1) + ... + f_p(x_p).
$$

$$\\[0.1in]$$




## Integrated Nested Laplace approximation (INLA)

**Integrated nested Laplace approximations (INLA)** is a method for approximate Bayesian inference based on **Laplace's method**. It is designed for a class of models called **latent Gaussian models (LGMs)**, for which it can be a fast and accurate alternative for MCMC methods to compute posterior marginal distributions.

**References**:<br>

- [Nice Overview](https://becarioprecario.bitbucket.io/inla-gitbook/ch-INLA.html#ref-INLAreview:2017)
- Rue, Havard; Martino, Sara; Chopin, Nicolas (2009). "Approximate Bayesian inference for latent Gaussian models by using integrated nested Laplace approximations". J. R. Statist. Soc. B. 71 (2): 319â€“392.

$$\\[0.1in]$$




## Variational method


Simple Monte-Carlo

Importance sampling

Acceptance-rejection methods
Adaptive rejection sampling

Markov chain Monte-Carlo (MCMC)
 Gibbs sampling
 Metropolis-Hastings sampling
 Slice sampling
 Hamiltonian Monte-Carlo
 Simulated annealing

Variable-dimensional MCMC - RJMCMC

Evaluating and comparing models

Bayesian nonparametrics
Bayesian optimization
Hierarchical models
Shrinkage priors
Spatial statistics
Graphical models
ABC methods
Perfect sampling

TMCMC
TTMCMC

Expectation propagation



Saurabh da undownloaded papers
------------------------------
2021 - Mukhopadhyay & Bhattacharya - Bayes factor asymptotics for variable selection in the Gaussian process framework
2021 - Guha & Bhattacharya - Bayesian Modeling of Discrete-Time Point-Referenced Spatio-Temporal Data
2022 - Chandra & Bhattacharya - Dependent Bayesian multiple hypothesis testing [Chapter]
2020 - Maitra & Bhattacharya - Asymptotic Theory of Bayes Factor in Stochastic Differential Equations with Increasing Number of Individuals [Chapter]





Adaptive MCMC
Ensemble MCMC
Random Walk MCMC
Component-wise MCMC
Multiple-Try MCMC



Bayesian deep learning: Deep learning is a popular technique for machine learning and has shown remarkable success in various applications, such as image recognition and natural language processing. Bayesian deep learning combines the flexibility of deep learning with the probabilistic framework of Bayesian statistics. This approach can provide more uncertainty estimates and more robust predictions.
Approximate Bayesian computation: Approximate Bayesian computation (ABC) is a method for Bayesian inference when the likelihood function is intractable or unknown. ABC uses a simulation-based approach to approximate the likelihood function, which enables the use of Bayesian methods even in complex and high-dimensional problems.
Bayesian causal inference: Causal inference is an important problem in many fields, including epidemiology, social sciences, and economics. Bayesian causal inference provides a framework for estimating causal effects and making causal inferences using Bayesian methods. This approach can provide more robust and informative results compared to traditional methods.
Bayesian nonparametric methods: Bayesian nonparametric methods are a set of techniques for statistical inference and machine learning that do not require a fixed set of parameters or assumptions about the underlying distribution. These methods can be useful in problems where the data may have complex or unknown distributions.
Bayesian optimization: Bayesian optimization is a method for optimizing complex and expensive-to-evaluate functions. This approach uses Bayesian methods to build a probabilistic model of the function, which is then used to guide the search for the optimal solution. Bayesian optimization can be used in various applications, such as hyperparameter tuning in machine learning models and experimental design in scientific research.






































